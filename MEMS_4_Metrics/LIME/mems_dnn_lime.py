# -*- coding: utf-8 -*-
"""mems dnn lime.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yZXWGu_VySc1YTuQeDnK3J782u3C7r1r
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from lime.lime_tabular import LimeTabularExplainer
import time
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

# Start timing
start_time = time.time()

# Combine feature columns with the label column
req_cols = ['x','y','z','label']
num_columns = 3
num_labels = 3

fraction = 0.5  # how much of that database you want to use
frac_normal = 0.2  # how much of the normal classification you want to reduce
split = 0.70  # how you want to split the train/test data (this is percentage for train)

# Model Parameters
batch_size = 32
epochs = 10

# XAI Samples
samples = 1000

# Specify the name of the output text file
output_file_name = "mems_DNN_LIME_output.txt"

print('--------------------------------------------------')
print('DNN')
print('--------------------------------------------------')
print('Importing Libraries')
print('--------------------------------------------------')

# Load your dataset
df = pd.read_csv('mems_dataset.csv', usecols=req_cols)

# Separate features (X) and labels (y)
X = df.drop(columns=['label'])
y = df['label']

# Map labels from 1 to 6 to 0 to 5
y -= 1

# Train and test split
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=split, random_state=42)

# Define the model
model = Sequential([
    Dense(64, activation='relu', input_shape=(num_columns,)),
    Dense(64, activation='relu'),
    Dense(num_labels, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)

# Calculate accuracy
accuracy = model.evaluate(X_test, y_test, verbose=0)[1]

# Create a function to predict probabilities using the trained model
def predict_proba(X):
    return model.predict(X)

# Create a Lime explainer object
explainer = LimeTabularExplainer(X_train.values, feature_names=X_train.columns.values, class_names=np.unique(y_train),
                                 discretize_continuous=True)

# Generate Lime explanations for each sample in the test set
lime_results = []
for i in range(samples):
    exp = explainer.explain_instance(X_test.iloc[i].values, predict_proba, num_features=num_columns,
                                     top_labels=num_labels)
    lime_list = exp.as_list()
    lime_list.sort()
    lime_results.append(lime_list)

    print('Progress:', 100 * (i + 1) / samples, '%')

# Sort and print feature importance in descending order
sorted_features = sorted(lime_results[0], key=lambda x: abs(x[1]), reverse=True)
print('Feature Importance (Descending Order):')
for feature, importance in sorted_features:
    print(feature, importance)

# Write feature importance to output file
with open(output_file_name, "a") as f:
    print('\nFeature Importance (Descending Order):', file=f)
    for feature, importance in sorted_features:
        print(feature, importance, file=f)

# Calculate and print sparsity values
sparsity_values = []
for threshold in np.arange(0.1, 1.1, 0.1):
    count_below_threshold = sum(1 for value in sorted_features if abs(value[1]) < threshold)
    sparsity_values.append(count_below_threshold / len(sorted_features))
    print('Sparsity at threshold {}: {}'.format(threshold, sparsity_values[-1]))

# Write sparsity values to output file
with open(output_file_name, "a") as f:
    print('\nSparsity Values:', file=f)
    for threshold, sparsity in zip(np.arange(0.1, 1.1, 0.1), sparsity_values):
        print('Threshold {}: {}'.format(threshold, sparsity), file=f)

# Write other information to output file
with open(output_file_name, "a") as f:
    print('Samples:', samples, file=f)
    print('Accuracy:', accuracy, file=f)

# End timing
end_time = time.time()

# Calculate execution time
execution_time = end_time - start_time

# Calculate execution time in hours
execution_time_hours = execution_time / 3600  # 3600 seconds in an hour

# Print execution time in hours
print("Execution time: %s hours" % execution_time_hours)

# Write execution time to output file in hours
with open(output_file_name, "a") as f:
    print('Execution time:', execution_time_hours, 'hours', file=f)

pip install lime

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from lime.lime_tabular import LimeTabularExplainer
import time
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

# Start timing
start_time = time.time()

# Combine feature columns with the label column
req_cols = ['x','y','z','label']
num_columns = 3
num_labels = 3

fraction = 0.5  # how much of that database you want to use
frac_normal = 0.2  # how much of the normal classification you want to reduce
split = 0.70  # how you want to split the train/test data (this is percentage for train)

# Model Parameters
batch_size = 32
epochs = 10

# XAI Samples
samples = 1000

# Specify the name of the output text file
output_file_name = "mems_DNN_LIME_output.txt"
with open(output_file_name, "a") as f:
    print('--------------------------------------------------', file=f)
    print('start', file=f)

print('--------------------------------------------------')
print('DNN')
print('--------------------------------------------------')
print('Importing Libraries')
print('--------------------------------------------------')

# Load your dataset
df = pd.read_csv('mems_dataset.csv', usecols=req_cols)

# Separate features (X) and labels (y)
X = df.drop(columns=['label'])
y = df['label']

# Map labels from 1 to 6 to 0 to 5
y -= 1

# Train and test split
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=split, random_state=42)

# Define the model
model = Sequential([
    Dense(64, activation='relu', input_shape=(num_columns,)),
    Dense(64, activation='relu'),
    Dense(num_labels, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)

# Calculate accuracy
accuracy = model.evaluate(X_test, y_test, verbose=0)[1]

# Create a function to predict probabilities using the trained model
def predict_proba(X):
    return model.predict(X)

# Create a Lime explainer object
explainer = LimeTabularExplainer(X_train.values, feature_names=X_train.columns.values, class_names=np.unique(y_train),
                                 discretize_continuous=True)

# Generate Lime explanations for each sample in the test set
for i in range(samples):
    exp = explainer.explain_instance(X_test.iloc[i].values, predict_proba, num_features=num_columns,
                                     top_labels=num_labels)
    lime_list = exp.as_list()
    lime_list.sort()
    print(lime_list)

    print('Progress:', 100 * (i + 1) / samples, '%')

print('---------------------------------------------------------------------------------')
print('Generating Explainer')
print('---------------------------------------------------------------------------------')

start = time.time()

explainer = LimeTabularExplainer(X_train.to_numpy(), feature_names=list(X_train.columns.values),
                                                  class_names=np.unique(y_train), discretize_continuous=True)

feat_list = req_cols[:-1]

feat_dict = dict.fromkeys(feat_list, 0)

c = 0

num_columns = df.shape[1] - 1
feature_name = req_cols[:-1]
feature_name.sort()
feature_val = []

for i in range(0, num_columns):
    feature_val.append(0)

for i in range(0, samples):
    exp = explainer.explain_instance(X_test.iloc[i].values, predict_proba, num_features=num_columns,
                                     top_labels=num_columns)

    lime_list = exp.as_list()
    lime_list.sort()
    print(lime_list)

    for j in range(0, num_columns):
        feature_val[j] += abs(lime_list[j][1])

    c = c + 1
    print('progress', 100 * (c / samples), '%')

divider = samples
feature_val = [x / divider for x in feature_val]

zipped_lists = list(zip(feature_name, feature_val))
zipped_lists.sort(key=lambda x: x[1], reverse=True)

sorted_list1, sorted_list2 = [list(x) for x in zip(*zipped_lists)]

print('----------------------------------------------------------------------------------------------------------------')

for item1, item2 in zip(sorted_list1, sorted_list2):
    print(item1, item2)
    with open(output_file_name, "a") as f:
        print(item1, item2, file=f)

for k in sorted_list1:
    with open(output_file_name, "a") as f:
        print("df.pop('", k, "')", sep='', file=f)

with open(output_file_name, "a") as f:
    print("Trial_ =[", file=f)
for k in sorted_list1:
    with open(output_file_name, "a") as f:
        print("'", k, "',", sep='', file=f)
with open(output_file_name, "a") as f:
    print("]", file=f)

print('---------------------------------------------------------------------------------')

end = time.time()
with open(output_file_name, "a") as f:
    print('ELAPSE TIME LIME GLOBAL: ', (end - start) / 60, 'min', file=f)
print('---------------------------------------------------------------------------------')

print('---------------------------------------------------------------------------------')
print('Generating Sparsity Graph')
print('---------------------------------------------------------------------------------')
print('')

# Calculate normalized list
min_value = min(feature_val)
max_value = max(feature_val)
normalized_list = [(x - min_value) / (max_value - min_value) for x in feature_val]

Sparsity = count_below_threshold / len(normalized_list)
Spar = []
print('Sparsity = ', Sparsity)
X_axis = []

# Corrected logic for calculating Sparsity and Spar
for i in range(0, 11):
    threshold = i / 10
    count_below_threshold = sum(1 for value in normalized_list if value < threshold)
    Sparsity = count_below_threshold / len(normalized_list)
    Spar.append(Sparsity)
    X_axis.append(threshold)
    print('Progress:', 100 * (i + 1) / 11, '%')

with open(output_file_name, "a") as f:
    print('y_axis_DNN = ', Spar, '', file=f)
with open(output_file_name, "a") as f:
    print('x_axis_DNN = ', X_axis, '', file=f)

plt.plot(X_axis, Spar, marker='o', linestyle='-')

plt.xlabel('X-Axis')
plt.ylabel('Y-Axis')

plt.title('Values vs. X-Axis')

plt.savefig('mems_sparsity_DNN_LIME.png')
plt.clf()

feature_val = [x / samples for x in feature_val]

zipped_lists = list(zip(X_train.columns.values, feature_val))
zipped_lists.sort(key=lambda x: x[1], reverse=True)

sorted_list1, sorted_list2 = [list(x) for x in zip(*zipped_lists)]

print('Feature Importance (Descending Order):')
for k, v in zip(sorted_list1, sorted_list2):
    print(k, v)

thresholds = [i / 10 for i in range(11)]
sparsity_values = []

for threshold in thresholds:
    count_below_threshold = sum(1 for value in feature_val if value < threshold)
    sparsity_values.append(count_below_threshold / len(feature_val))

print('Sparsity:', sparsity_values)

plt.plot(thresholds, sparsity_values, marker='o', linestyle='-')
plt.xlabel('Threshold')
plt.ylabel('Sparsity')
plt.title('Sparsity vs. Threshold')
plt.savefig('mems_sparsity_DNN_LIME.png')
plt.clf()

with open(output_file_name, "a") as f:
    print('\n--------------------------------------------------', file=f)
    print('DNN', file=f)
    print('--------------------------------------------------', file=f)
    print('Feature Importance (Descending Order):', file=f)
    for k, v in zip(sorted_list1, sorted_list2):
        print(k, v, file=f)
    print('Accuracy:', accuracy, file=f)
    print('Sparsity:', sparsity_values, file=f)
    print('Samples:', samples, file=f)

end_time = time.time()

execution_time = end_time - start_time

execution_time_hours = execution_time / 3600

print("Execution time: %s hours" % execution_time_hours)

with open(output_file_name, "a") as f:
    print('Execution time:', execution_time_hours, 'hours', file=f)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from lime.lime_tabular import LimeTabularExplainer
import time
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

# Start timing
start_time = time.time()

# Combine feature columns with the label column
req_cols = ['x', 'y', 'z', 'label']
num_columns = 3
num_labels = 3

fraction = 0.5  # how much of that database you want to use
frac_normal = 0.2  # how much of the normal classification you want to reduce
split = 0.70  # how you want to split the train/test data (this is percentage for train)

# Model Parameters
batch_size = 32
epochs = 10

# XAI Samples
samples = 1000

# Specify the name of the output text file
output_file_name = "mems_DNN_LIME_output.txt"
with open(output_file_name, "a") as f:
    print('--------------------------------------------------', file=f)
    print('start', file=f)

print('--------------------------------------------------')
print('DNN')
print('--------------------------------------------------')
print('Importing Libraries')
print('--------------------------------------------------')

# Load your dataset
df = pd.read_csv('mems_dataset.csv', usecols=req_cols)

# Separate features (X) and labels (y)
X = df.drop(columns=['label'])
y = df['label']

# Map labels from 1 to 6 to 0 to 5
y -= 1

# Train and test split
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=split, random_state=42)

# Define the model
model = Sequential([
    Dense(64, activation='relu', input_shape=(num_columns,)),
    Dense(64, activation='relu'),
    Dense(num_labels, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)

# Calculate accuracy
accuracy = model.evaluate(X_test, y_test, verbose=0)[1]

# Create a function to predict probabilities using the trained model
def predict_proba(X):
    return model.predict(X)

# Create a Lime explainer object
explainer = LimeTabularExplainer(X_train.values, feature_names=X_train.columns.values, class_names=np.unique(y_train),
                                 discretize_continuous=True)

# Generate Lime explanations for each sample in the test set
for i in range(samples):
    exp = explainer.explain_instance(X_test.iloc[i].values, predict_proba, num_features=num_columns,
                                     top_labels=num_labels)
    lime_list = exp.as_list()
    lime_list.sort()
    print(lime_list)

    print('Progress:', 100 * (i + 1) / samples, '%')

print('---------------------------------------------------------------------------------')
print('Generating Explainer')
print('---------------------------------------------------------------------------------')

start = time.time()

explainer = LimeTabularExplainer(X_train.to_numpy(), feature_names=list(X_train.columns.values),
                                                  class_names=np.unique(y_train), discretize_continuous=True)

feat_list = req_cols[:-1]

feat_dict = dict.fromkeys(feat_list, 0)

c = 0

num_columns = df.shape[1] - 1
feature_name = req_cols[:-1]
feature_name.sort()
feature_val = []

for i in range(0, num_columns):
    feature_val.append(0)

for i in range(0, samples):
    exp = explainer.explain_instance(X_test.iloc[i].values, predict_proba, num_features=num_columns,
                                     top_labels=num_columns)

    lime_list = exp.as_list()
    lime_list.sort()
    print(lime_list)

    for j in range(0, num_columns):
        feature_val[j] += abs(lime_list[j][1])

    c = c + 1
    print('progress', 100 * (c / samples), '%')

divider = samples
feature_val = [x / divider for x in feature_val]

zipped_lists = list(zip(feature_name, feature_val))
zipped_lists.sort(key=lambda x: x[1], reverse=True)

sorted_list1, sorted_list2 = [list(x) for x in zip(*zipped_lists)]

print('----------------------------------------------------------------------------------------------------------------')

for item1, item2 in zip(sorted_list1, sorted_list2):
    print(item1, item2)
    with open(output_file_name, "a") as f:
        print(item1, item2, file=f)

for k in sorted_list1:
    with open(output_file_name, "a") as f:
        print("df.pop('", k, "')", sep='', file=f)

with open(output_file_name, "a") as f:
    print("Trial_ =[", file=f)
for k in sorted_list1:
    with open(output_file_name, "a") as f:
        print("'", k, "',", sep='', file=f)
with open(output_file_name, "a") as f:
    print("]", file=f)

print('---------------------------------------------------------------------------------')

end = time.time()
with open(output_file_name, "a") as f:
    print('ELAPSE TIME LIME GLOBAL: ', (end - start) / 60, 'min', file=f)
print('---------------------------------------------------------------------------------')

print('---------------------------------------------------------------------------------')
print('Generating Sparsity Graph')
print('---------------------------------------------------------------------------------')
print('')

# Calculate normalized list
min_value = min(feature_val)
max_value = max(feature_val)
normalized_list = [(x - min_value) / (max_value - min_value) for x in feature_val]

Sparsity = count_below_threshold / len(normalized_list)
Spar = []
print('Sparsity = ', Sparsity)
X_axis = []

# Corrected logic for calculating Sparsity and Spar
for i in range(0, 11):
    threshold = i / 10
    count_below_threshold = sum(1 for value in normalized_list if value < threshold)
    Sparsity = count_below_threshold / len(normalized_list)
    Spar.append(Sparsity)
    X_axis.append(threshold)
    print('Progress:', 100 * (i + 1) / 11, '%')

with open(output_file_name, "a") as f:
    print('y_axis_DNN = ', Spar, '', file=f)
with open(output_file_name, "a") as f:
    print('x_axis_DNN = ', X_axis, '', file=f)

plt.plot(X_axis, Spar, marker='o', linestyle='-')

plt.xlabel('X-Axis')
plt.ylabel('Y-Axis')

plt.title('Values vs. X-Axis')

plt.savefig('mems_sparsity_DNN_LIME.png')
plt.clf()

feature_val = [x / samples for x in feature_val]

zipped_lists = list(zip(X_train.columns.values, feature_val))
zipped_lists.sort(key=lambda x: x[1], reverse=True)

sorted_list1, sorted_list2 = [list(x) for x in zip(*zipped_lists)]

print('Feature Importance (Descending Order):')
for k, v in zip(sorted_list1, sorted_list2):
    print(k, v)

thresholds = [i / 10 for i in range(11)]
sparsity_values = []

for threshold in thresholds:
    count_below_threshold = sum(1 for value in feature_val if value < threshold)
    sparsity_values.append(count_below_threshold / len(feature_val))

print('Sparsity:', sparsity_values)

plt.plot(thresholds, sparsity_values, marker='o', linestyle='-')
plt.xlabel('Threshold')
plt.ylabel('Sparsity')
plt.title('Sparsity vs. Threshold')
plt.savefig('mems_sparsity_DNN_LIME.png')
plt.clf()

with open(output_file_name, "a") as f:
    print('\n--------------------------------------------------', file=f)
    print('DNN', file=f)
    print('--------------------------------------------------', file=f)
    print('Feature Importance (Descending Order):', file=f)
    for k, v in zip(sorted_list1, sorted_list2):
        print(k, v, file=f)
    print('Accuracy:', accuracy, file=f)
    print('Sparsity:', sparsity_values, file=f)
    print('Samples:', samples, file=f)

end_time = time.time()

execution_time = end_time - start_time

execution_time_hours = execution_time / 3600

print("Execution time: %s hours" % execution_time_hours)

with open(output_file_name, "a") as f:
    print('Execution time:', execution_time_hours, 'hours', file=f)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from lime.lime_tabular import LimeTabularExplainer
import time
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

# Start timing
start_time = time.time()

# Combine feature columns with the label column
req_cols = ['x', 'y', 'z', 'label']
num_columns = 3
num_labels = 3

fraction = 0.5  # how much of that database you want to use
frac_normal = 0.2  # how much of the normal classification you want to reduce
split = 0.70  # how you want to split the train/test data (this is percentage for train)

# Model Parameters
batch_size = 32
epochs = 10

# XAI Samples
samples = 2500


# Specify the name of the output text file
output_file_name = "mems_DNN_LIME_output.txt"
with open(output_file_name, "a") as f:
    print('--------------------------------------------------', file=f)
    print('start', file=f)

print('--------------------------------------------------')
print('DNN')
print('--------------------------------------------------')
print('Importing Libraries')
print('--------------------------------------------------')

# Load your dataset
df = pd.read_csv('mems_dataset.csv', usecols=req_cols)

# Separate features (X) and labels (y)
X = df.drop(columns=['label'])
y = df['label']

# Map labels from 1 to 6 to 0 to 5
y -= 1

# Train and test split
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=split, random_state=42)

# Define the model
model = Sequential([
    Dense(64, activation='relu', input_shape=(num_columns,)),
    Dense(64, activation='relu'),
    Dense(num_labels, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)

# Calculate accuracy
accuracy = model.evaluate(X_test, y_test, verbose=0)[1]

# Create a function to predict probabilities using the trained model
def predict_proba(X):
    return model.predict(X)

# Create a Lime explainer object
explainer = LimeTabularExplainer(X_train.values, feature_names=X_train.columns.values, class_names=np.unique(y_train),
                                 discretize_continuous=True)

# Generate Lime explanations for each sample in the test set
for i in range(samples):
    exp = explainer.explain_instance(X_test.iloc[i].values, predict_proba, num_features=num_columns,
                                     top_labels=num_labels)
    lime_list = exp.as_list()
    lime_list.sort()
    print(lime_list)

    print('Progress:', 100 * (i + 1) / samples, '%')

print('---------------------------------------------------------------------------------')
print('Generating Explainer')
print('---------------------------------------------------------------------------------')

start = time.time()

explainer = LimeTabularExplainer(X_train.to_numpy(), feature_names=list(X_train.columns.values),
                                                  class_names=np.unique(y_train), discretize_continuous=True)

feat_list = req_cols[:-1]

feat_dict = dict.fromkeys(feat_list, 0)

c = 0

num_columns = df.shape[1] - 1
feature_name = req_cols[:-1]
feature_name.sort()
feature_val = []

for i in range(0, num_columns):
    feature_val.append(0)

for i in range(0, samples):
    exp = explainer.explain_instance(X_test.iloc[i].values, predict_proba, num_features=num_columns,
                                     top_labels=num_columns)

    lime_list = exp.as_list()
    lime_list.sort()
    print(lime_list)

    for j in range(0, num_columns):
        feature_val[j] += abs(lime_list[j][1])

    c = c + 1
    print('progress', 100 * (c / samples), '%')

divider = samples
feature_val = [x / divider for x in feature_val]

zipped_lists = list(zip(feature_name, feature_val))
zipped_lists.sort(key=lambda x: x[1], reverse=True)

sorted_list1, sorted_list2 = [list(x) for x in zip(*zipped_lists)]

print('----------------------------------------------------------------------------------------------------------------')

for item1, item2 in zip(sorted_list1, sorted_list2):
    print(item1, item2)
    with open(output_file_name, "a") as f:
        print(item1, item2, file=f)

for k in sorted_list1:
    with open(output_file_name, "a") as f:
        print("df.pop('", k, "')", sep='', file=f)

with open(output_file_name, "a") as f:
    print("Trial_ =[", file=f)
for k in sorted_list1:
    with open(output_file_name, "a") as f:
        print("'", k, "',", sep='', file=f)
with open(output_file_name, "a") as f:
    print("]", file=f)

print('---------------------------------------------------------------------------------')

end = time.time()
with open(output_file_name, "a") as f:
    print('ELAPSE TIME LIME GLOBAL: ', (end - start) / 60, 'min', file=f)
print('---------------------------------------------------------------------------------')

print('---------------------------------------------------------------------------------')
print('Generating Sparsity Graph')
print('---------------------------------------------------------------------------------')
print('')

# Removed sparsity calculation

with open(output_file_name, "a") as f:
    print('\n--------------------------------------------------', file=f)
    print('DNN', file=f)
    print('--------------------------------------------------', file=f)
    print('Accuracy:', accuracy, file=f)
    print('Samples:', samples, file=f)

end_time = time.time()

execution_time = end_time - start_time

execution_time_hours = execution_time / 3600

print("Execution time: %s hours" % execution_time_hours)

with open(output_file_name, "a") as f:
    print('Execution time:', execution_time_hours, 'hours', file=f)