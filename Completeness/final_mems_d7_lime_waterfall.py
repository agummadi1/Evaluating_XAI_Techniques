# -*- coding: utf-8 -*-
"""final mems/d7 lime waterfall.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10wxwqJMqhQa8Z8S5Ty-rKFznPUFtLvS5
"""

pip install lime

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc
from sklearn.preprocessing import label_binarize
from sklearn.metrics import f1_score, accuracy_score
from sklearn.utils import shuffle
from imblearn.over_sampling import RandomOverSampler
import lime
import time
from collections import Counter
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import lime.lime_tabular
from IPython.display import display

# Define oversampling function
def oversample(X, y):
    ros = RandomOverSampler(random_state=42)
    X_resampled, y_resampled = ros.fit_resample(X, y)
    return X_resampled, y_resampled

# Load dataset
data = pd.read_csv("device7_top_20_features.csv")

# data = pd.read_csv("mems_dataset.csv")
# data.pop('time')

y = data.pop('label')

print('---------------------------------------------------------------------------------')
print('Normalizing database')
print('---------------------------------------------------------------------------------')
print('')


# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Fit and transform the DataFrame
scaled_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)
data = scaled_data
data = data.assign( label = y)

# Rename labels for better readability
label_map = {1: 'benign', 2: 'gafgyt.combo', 3: 'gafgyt.junk', 4: 'gafgyt.scan', 5: 'gafgyt.tcp', 6: 'gafgyt.udp'} # d7 change
#label_map = {1: 'Normal', 2: 'Near-failure', 3: 'Failure'}

data['label'] = data['label'].map(label_map)

# Separate features and labels
X = data.drop('label', axis=1)
# X = data[['x','y','z']]

y = data['label']

# Define model parameters
max_depth = 5
n_estimators = 5
min_samples_split = 2

# Define XAI parameters
output_file_name = "final_Completeness_d7_LIME.txt" # mems rename

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize Random Forest classifier
rf_classifier = RandomForestClassifier(max_depth=max_depth, n_estimators=n_estimators, min_samples_split=min_samples_split, n_jobs=-1)

# Initialize SHAP explainer
explainer = None

# Model training
print('Training the model')
start = time.time()
rf_classifier.fit(X_train, y_train)
end = time.time()
print('ELAPSED TIME MODEL TRAINING:', (end - start) / 60, 'min')

# Model prediction
print('Predicting using the model')
start = time.time()
y_pred = rf_classifier.predict(X_test)
end = time.time()
print('ELAPSED TIME MODEL PREDICTION:', (end - start) / 60, 'min')

# Evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')
conf_matrix = confusion_matrix(y_test, y_pred)
roc_auc = roc_auc_score(label_binarize(y_test, classes=['benign','gafgyt.combo','gafgyt.junk','gafgyt.scan','gafgyt.tcp','gafgyt.udp']),
                        label_binarize(y_pred, classes=['benign','gafgyt.combo','gafgyt.junk','gafgyt.scan','gafgyt.tcp','gafgyt.udp']), average='macro')

# roc_auc = roc_auc_score(label_binarize(y_test, classes=['Normal', 'Near-failure', 'Failure']),
#                         label_binarize(y_pred, classes=['Normal', 'Near-failure', 'Failure']), average='macro')
print('Process Completed')

X_test

label = ['benign','gafgyt.combo','gafgyt.junk','gafgyt.scan','gafgyt.tcp','gafgyt.udp']
# label = ['Normal','Near-failure', 'Failure']

req_cols = X_test.columns
df = data
test2 = X_test
test = X_test.to_numpy()

#Define function to test sample with the waterfall plot
def lime_explanator(sample):
    # print(sample)
    sample_df = sample
    sample = sample.to_numpy()
    # print(sample)
    sample = sample[0]
    # print(sample)

    explainer = lime.lime_tabular.LimeTabularExplainer(test, feature_names= list(test2.columns.values) , class_names=label , discretize_continuous=True)

    #creating dict
    feat_list = req_cols[:-1]
    # print(feat_list)

    c = 0

    num_columns = df.shape[1]- 1
    feature_name = req_cols
    feature_name.sort_values()
    # print('features after sorting:', feature_name.sort_values())
    feature_val = []
    feature_val_abs = []
    samples = 1
    # position = y_labels.index(rf.predict(Dos_sample2))

    position =  np.argmax(rf_classifier.predict_proba(((sample_df))))
    prediction = label[position]
    # print(len(y_labels))
    # print(rf.predict(Dos_sample2))


    # sample = Dos_sample
    # sample = Normal_sample
    # sample = PS_sample


    for i in range(0,num_columns):
        feature_val.append(0)
        feature_val_abs.append(0)

    # for i in range(0,samples):
    #   i = sample
    #   exp = explainer.explain_instance(test[i], rf_classifier.predict_proba)

    exp = explainer.explain_instance(sample, rf_classifier.predict_proba, num_features=num_columns, top_labels=len(label))
    exp.show_in_notebook(show_table=True, show_all=True) # figure

    lime_list = exp.as_list(position)
    lime_list.sort()
    # print(lime_list)

    for i in range(0,len(lime_list)):
        #---------------------------------------------------
        #fix
        my_string = lime_list[i][0]
        for index, char in enumerate(my_string):
            if char.isalpha():
                first_letter_index = index
                break  # Exit the loop when the first letter is found

        my_string = my_string[first_letter_index:]
        modified_tuple = list(lime_list[i])
        modified_tuple[0] = my_string
        lime_list[i] = tuple(modified_tuple)

        #---------------------------------------------------

    lime_list.sort()
    # for j in range (0,num_columns): feature_val[j]+= abs(lime_list[j][1])
    for j in range (0,num_columns):feature_val_abs[j] = abs(lime_list[j][1])
    for j in range (0,num_columns):feature_val[j] = lime_list[j][1]
    c = c + 1
    # print ('progress',100*(c/samples),'%')
    # print('abs',feature_val_abs)
    # print('val',feature_val)

    # Define the number you want to divide by
    # divider = samples

    # Use a list comprehension to divide all elements by the same number
    # feature_val = [x / divider for x in feature_val]

    # for item1, item2 in zip(feature_name, feature_val):
    #     print(item1, item2)

    # Use zip to combine the two lists, sort based on list1, and then unzip them
    zipped_lists = list(zip(feature_name, feature_val,feature_val_abs))
    zipped_lists.sort(key=lambda x: x[2],reverse=True)

    print('zipped lists:', zipped_lists)

    # Convert the sorted result back into separate lists
    sorted_list1, sorted_list2,sorted_list3 = [list(x) for x in zip(*zipped_lists)]
    feature_name = sorted_list1
    lime_val = sorted_list2
    # print(sorted_list1)
    # print(sorted_list2)
    # print(sorted_list3)
    return (prediction, lime_val,feature_name)

X_test

sample = X_test[0:1] #original run mems
print(sample)

lime_explanator(sample)

sample['z'] = 0.40 #p1 mems
print(sample)

lime_explanator(sample)

sample['z'] = 0.72 #p2 mems
print(sample)

lime_explanator(sample)

sample = X_test[0:1] #original run d7
print(sample)

lime_explanator(sample)

sample['HH_L5_magnitude'] = 0.01 #p1 d7
print(sample)

lime_explanator(sample)